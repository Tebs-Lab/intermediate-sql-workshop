# Exercise: Discuss Data Collection

This will be the first of many discussion sections. We will repeatedly discuss the following three companies as archetypes of unique database needs and concerns. The purpose of all these exercises is to help you think about:

* The wide variety of needs and concerns of companies as it relates to data storage.
* The decisions and tradeoffs that database administrators must make with respect to these concerns.
* The impact those decisions have on downstream users of the data.

Each discussion section will focus on the same three companies, but different aspects of their data storage needs. 

## Discussion

1. What are the most important sources of data for this company?
    1. Consider manual and automated processes.
    2. Are these sources tightly controlled by the company?
2. How might this data get processed and stored?
    1. Is the data clean, accurate, and in the proper format when it arrives?
    2. Is other software needed to translate the raw data into a SQL compatible format?
3. If you were to administer these databases, what challenges would you expect with respect to the sources of data and the potential ETL processes?

### Starbucks 

* Tightly controlled
    * Point of Sales, custom software to transform.
    * Customer data from web/phone apps, app/server would ensure the data is already in a proper format.
* Loosely controlled/require ETL processes/potentially erronious
    * Probably a lot of 3rd party data from logistics and supplier partners.
    * Probably 3rd party sales data.
* Major challenges:
    * So many stores all processing transactions all around the world constantly is a lot of load. 
        * Managing these connections and the geographic distribution might be hard.
        * Also, POS systems aren't always known for their wonderful interfaces and APIs... getting them to play nice could be hard.
        * Note: Write heavy workload, not so many reads.
        * Note: Reads don't really need to be "up to the second" so separate OLTP/OLAP database with nightly dump could work well.
    * Getting suppliers to conform to a data format could really lessen the pain from 3rd party sources, otherwise ETL could get out of hand.

### Twitter

* Pretty much all the relevant data is generated by the web application or phone applications directly.
    * The stuff that isn't is mediated by their API. 
    * So all of it is really tightly controlled. (lucky!)
* I wouldn't expect much ETL at Twitter.
* Major challenges:
    * So many reads and writes all the time from around the world.
    * Tweets need to be broadcast in realtime, often to millions of users.
    * Need a database system that can handle constant writing without locking tables.
    * Need a database system that can be efficiently read AND searched by text.
    * All of this has to happen in real time, so separating out the OLTP system from the system that publishes and searches through tweets isn't realistic.

### Kaiser Permanente

* Most critical data will be generated by medical software integrations with an Electronic Medical Records System.
    * These are well controlled by the company.
    * Mix of manual (doctor fills a form) and automatic (e.g. MRI scan should be automatically stored)
* Some will come from a web interface, e.g. from patients directly.
    * But still, this app and server are controlled by the company.
* But, many records may come from other hospital systems from when people move, change providers, etc.
    * All external EMS systems will require some ETL, and are not tightly controlled.
* My primary concerns as a DBA would be legal, regulatory, and security based concerns. 
    * Ensuring all of the connections between our database are secure.
    * Ensuring data doesn't leak from our database to insecure systems during the ETL process.
    * Ensuring the ETL processes don't "hang onto" data in the form of system logs etc.
* Note: The amount of data and the speed is really not at huge scale, esp. compared to Twitter or SB.
    * Medical transactions are at slower speeds, and Kaiser serves a smaller market.

## The Exercise, Technical

In the data folder of this repo you'll find a file called `sample-import.xlsx`. It is a mock spreadsheet that AdventureWorks has "received" from one of its vendors. The spreadsheet represents  updates to this vendor's min and max order quantities.

Devise and enact a strategy to import this data such that it properly updates rows in the ProductVendor table of the AdventureWorks database. In devising a strategy consider the following...

1. Can you completely automate this?
    * **Maybe...**, 
    * If this were always the format, this could be automated with the help of additional software that can parse XLSX files. Any changes to the format will break the integration.
    * BUT, the product names might not be unique enough to always unique distinguish them, which would prevent a successful automation.
2. Can you partially automate this, or reduce the number of times you have to initiate a database query to 1?
    * Not really, because the product names are in a different table than the all the other values, so we have to query Product to find the ProductID in order to update the proper rows in ProductVendor
    * Same goes for the name and ID of the actual Vendor.


The minimum amount of work I think you need to successfully update:
```sql
# Deterimine the business entity id of "Compete, Inc"
select name, businessentityid from purchasing.vendor where "name" like 'Compete%';

# Find the mapping of product names to products
select 
  p.name,
  p.productid 
from purchasing.vendor v
join purchasing.productvendor pv on pv.businessentityid=v.businessentityid
join production.product p on p.productid=pv.productid
where v.businessentityid=1694;

# Update the individual rows.
update purchasing.productvendor SET (averageleadtime, standardprice, minorderqty, maxorderqty, onorderqty) = (10, 7, 200, 1000, 300) where productid=351 and businessentityid=1694;
update purchasing.productvendor SET (averageleadtime, standardprice, minorderqty, maxorderqty, onorderqty) = (30, 1.3, 50, 1000, NULL) where productid=352 and businessentityid=1694;
update purchasing.productvendor SET (averageleadtime, standardprice, minorderqty, maxorderqty, onorderqty) = (25, 6, 500, 1000, 300) where productid=679 and businessentityid=1694;

# Confirm it worked
select p.name, p.productid, pv.* from purchasing.vendor v
join purchasing.productvendor pv on pv.businessentityid=v.businessentityid
join production.product p on p.productid=pv.productid
where v.businessentityid=1694;
```


### Bonus Points

Assume you could impose arbitrary constraints on the vendor in question with regards to the format of this spreadsheet and the information contained therein. What would you have the vendor change and include in order to make this process more (fully) automated?

#### Solution:

The easiest thing would be to require that each row supplied by the vendor includes the matching ProductVendorID, then none of this exploratory work would be necessary and writing the updates would be highly automate-able with some fairly simple software. Additionally, specifying exactly which rows would appear in which order would make that automation even easier.